{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a causal language model from scratch\n",
    "\n",
    "In this section we will build a scaled-down version of a code generation model: weâ€™ll focus on one-line completions instead of full functions or classes, using a subset of Python code. When working with data in Python you are in frequent contact with the Python data science stack, consisting of the matplotlib, seaborn, pandas, and scikit-learn libraries. When using those frameworks itâ€™s common to need to look up specific commands, so it would be nice if we could use a model to complete these calls for us.\n",
    "\n",
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration huggingface-course--codeparrot-ds-train-7e9fc5dfe436a81a\n",
      "Found cached dataset json (/home/hujunhao/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-train-7e9fc5dfe436a81a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "Using custom data configuration huggingface-course--codeparrot-ds-valid-65557c3279496c87\n",
      "Found cached dataset json (/home/hujunhao/.cache/huggingface/datasets/huggingface-course___json/huggingface-course--codeparrot-ds-valid-65557c3279496c87/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "# First take out a small sample of the data to test the model\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train.shuffle().select(range(50000)),\n",
    "        \"valid\": ds_valid.shuffle().select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME: nvoron23/scikit-learn\n",
      "PATH: examples/svm/plot_separating_hyperplane_unbalanced.py\n",
      "COPIES: 329\n",
      "SIZE: 1850\n",
      "CONTENT: \"\"\"\n",
      "=================================================\n",
      "SVM: Separating hyperplane for unbalanced classes\n",
      "=================================================\n",
      "\n",
      "Find the optimal separating hyperplane using \n",
      "LICENSE: bsd-3-clause\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our goal is to mainly autocomplete short function calls, we can keep the context size relatively small. For now, letâ€™s fix the context size at 128 tokens, as opposed to the 1,024 or 2,048 used in GPT-2 or GPT-3, respectively. \n",
    "\n",
    "Most documents contain many more than 128 tokens, so simply truncating the inputs to the maximum length would eliminate a large fraction of our dataset. Instead, weâ€™ll use the return_overflowing_tokens option to tokenize the whole input and split it into several chunks.\n",
    "\n",
    "Weâ€™ll also use the return_length option to return the length of each created chunk automatically. Often the last chunk will be smaller than the context size, and weâ€™ll get rid of these pieces to avoid padding issues; we donâ€™t really need them as we have plenty of data anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be193beb68fe4d97abba66f610378e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a71b3bccfc4561b8f3c50a928e897c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1381061\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 16414\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model\n",
    "\n",
    "Our first step is to freshly initialize a GPT-2 model. Weâ€™ll use the same configuration for our model as for the small GPT-2 model, so we load the pretrained configuration, make sure that the tokenizer size matches the model vocabulary size and pass the bos and eos (beginning and end of sequence) token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.2M parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start training, we need to set up a data collator that will take care of creating the batches. We can use the DataCollatorForLanguageModeling collator, which is designed specifically for language modeling (as the name subtly suggests). Besides stacking and padding batches, it also takes care of creating the language model labels â€” in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training so we donâ€™t need to duplicate the input_ids.\n",
    "\n",
    "Note that DataCollatorForLanguageModeling supports both masked language modeling (MLM) and causal language modeling (CLM). By default it prepares data for MLM, but we can switch to CLM by setting the argument mlm=False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 128])\n",
      "attention_mask shape: torch.Size([5, 128])\n",
      "labels shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "# for check purpose\n",
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  280,   173, 47384,    29,   173, 46575,    26, 19089,  1443, 41476,\n",
       "           296,   674, 23614,  4025,   173, 47384,    29,   173,   173,  5274,\n",
       "           256, 11619, 27225, 41476,  1312,   309, 12350,    35,   296,  4025,\n",
       "           542,   173,  1404,   674, 23614,    14,   173,   173,  4392,  1165,\n",
       "          1588,   256, 27225,  7287,   461,   231,  8894, 12350,    35,   350,\n",
       "          1319,  1676,   173,     8, 29813,     9,   256, 27225, 41476,   461,\n",
       "          4322,  9807,   296,   173,   289, 23614,  4025,    14,   173,   173,\n",
       "           519,   966,  1299,  1125, 15673,    14,  5349,    63,   961,   173,\n",
       "           173,   519,  2719,  1125,   312,   841,  2228,   664,  2091,  1334,\n",
       "           585, 14874,   581, 24607,     8,  5087,   861,  5349, 34623,   232,\n",
       "           461,   581, 14843,    36, 16983,     8,  4287,   861,    72, 22547,\n",
       "           528,  1826, 10225,   256,   581,  4287,   423,  1449,   232,   311,\n",
       "           256,   310,   692,   749, 14843,    36, 16983,    64],\n",
       "        [ 3185,   292,   581,    72, 22547,   423,   664,  1483, 11082,   232,\n",
       "          2802,   442,   542,   311,   231, 12350,    35,   461,   231,  5273,\n",
       "          4044,    14,   312,  1652,  2228,   646,  2478,   311,   256,   581,\n",
       "         24607, 43513,   298, 23574,   233,   438, 17443, 16983,     8,    78,\n",
       "            63,   988,    29,  3087,    12,  3122,    29,    16,    14,  1057,\n",
       "             9,   173,   173,   280,   173,  1647,  4508,  1276,  3485,   173,\n",
       "           173,  2745,  1601,   442,   635,   173,  2745,  4855,    14, 11032,\n",
       "           442,  2564,   173,   973, 15673,   978, 36697,   173,     3,   973,\n",
       "         15673,    14,  5349,    63,   961,   978,   438, 17443, 16983,   173,\n",
       "           173,     3,   649,   969,  8102,  2756,   463,  2157,   173, 13404,\n",
       "           233,   635,    14,  2437,    14, 16864,     8,    16,     9,   173,\n",
       "            78,    63,  2461,    63,    17,   233,  4764,   173,    78,    63,\n",
       "          2461,    63,    18,   233,  3038,   173,    56,   233],\n",
       "        [  635,    14,    82,  7285,    17,    14,    21,   408,  9927,    14,\n",
       "         16670,     8,    78,    63,  2461,    63,    17,    12,   554,   491,\n",
       "           880,   443,    14,    21,   408,  9927,    14, 16670,     8,    78,\n",
       "            63,  2461,    63,    18,    12,   554,     9,   382,   404,    18,\n",
       "            12,   554,  1730,   173,    89,   233,   404,    16,    61,   408,\n",
       "           308,    78,    63,  2461,    63,    17,     9,   382,   404,    17,\n",
       "            61,   408,   308,    78,    63,  2461,    63,    18,     9,   173,\n",
       "           173,     3,  2872,   256,   937,   350,   549,   256, 27225, 41476,\n",
       "           173, 16158,   233, 36697,    14, 24607,     8,  5087,   567,  5349,\n",
       "           340,   447,    29,    17,    14,    16,     9,   173, 16158,    14,\n",
       "          2770,     8,    56,    12,   522,     9,   173,   173,    87,   233,\n",
       "         23574,    14,  7602,  7285,    16,    61,   173,    65,   233,   415,\n",
       "            87,    59,    16,    61,   823,   283,    59,    17],\n",
       "        [   61,   173,  3483,   233,   635,    14,  8042,  3226,    21,    12,\n",
       "          1462,     9,   173,  6997,   233,   231,   408, 12017,   415, 23574,\n",
       "            14, 13267,  7285,    16,    61,   823,   283,    59,    17,    61,\n",
       "          4391,   173,     3,   549,   256, 27225, 41476,  1312,  7932,  4025,\n",
       "           173, 15515,   247,   233, 36697,    14, 24607,     8,  5087,   567,\n",
       "          5349,   340,  1149,    63,  2077,  1970,    17,    26,  2009,  1583,\n",
       "           173, 15515,   247,    14,  2770,     8,    56,    12,   522,     9,\n",
       "           173,   173,  2360,   233,   283, 16158,    14,  7602,  7285,    16,\n",
       "            61,   173,   910,   233,   415,  2360,    59,    16,    61,   823,\n",
       "         31005,    59,    17,    61,   173,    87,  6997,   233,  5881,   408,\n",
       "         12017,   415,   283, 16158,    14, 13267,  7285,    16,    61,   823,\n",
       "         31005,    59,    17,    61,   173,   173,     3,  1676, 27225,  7103,\n",
       "         19754,   350,  2751,   173,    72,    16,   233,  2564],\n",
       "        [ 2421,    41,    13,  4609,  1799,  8043,  1942,   480,   461,  2405,\n",
       "          2164,   296,   256,   173,    37,    13,  4609,  1799,  1942,  1314,\n",
       "           461, 22055,  5491,    15,  7098, 11835,   292,   525,  5625,  5921,\n",
       "           173,   973,  1289,  5964,   612,   978,  4381,    63,  2745,    12,\n",
       "           781,    63,  1619,   173,   173,  2745,   756,    14,   419,   173,\n",
       "           173,   973, 36312,   978, 31780,   173,  2745,  4855,    14, 16936,\n",
       "           442, 13444,   173,   173,   973,  5599,  3468,    14, 33978,    14,\n",
       "           838,   978, 27484,  4859, 36664,  4391,   173,  2696,    63,  2841,\n",
       "           233,   396,    14,   173,   173,  3247,    63,  1547,   233,   396,\n",
       "            14,   408,  2814,    63,  2841,   173,  3247,    63,   563,   214,\n",
       "           233,  1911,    14,   408,  2814,    63,  2841,   173,   173,  8537,\n",
       "            63,  5202,   233,  1383, 14714,    63,   441,   340,   269,    73,\n",
       "            63,  4609,  1799,   340,   269,    69,    63,  4593]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  280,   173, 47384,    29,   173, 46575,    26, 19089,  1443, 41476,\n",
       "           296,   674, 23614,  4025,   173, 47384,    29,   173,   173,  5274,\n",
       "           256, 11619, 27225, 41476,  1312,   309, 12350,    35,   296,  4025,\n",
       "           542,   173,  1404,   674, 23614,    14,   173,   173,  4392,  1165,\n",
       "          1588,   256, 27225,  7287,   461,   231,  8894, 12350,    35,   350,\n",
       "          1319,  1676,   173,     8, 29813,     9,   256, 27225, 41476,   461,\n",
       "          4322,  9807,   296,   173,   289, 23614,  4025,    14,   173,   173,\n",
       "           519,   966,  1299,  1125, 15673,    14,  5349,    63,   961,   173,\n",
       "           173,   519,  2719,  1125,   312,   841,  2228,   664,  2091,  1334,\n",
       "           585, 14874,   581, 24607,     8,  5087,   861,  5349, 34623,   232,\n",
       "           461,   581, 14843,    36, 16983,     8,  4287,   861,    72, 22547,\n",
       "           528,  1826, 10225,   256,   581,  4287,   423,  1449,   232,   311,\n",
       "           256,   310,   692,   749, 14843,    36, 16983,    64],\n",
       "        [ 3185,   292,   581,    72, 22547,   423,   664,  1483, 11082,   232,\n",
       "          2802,   442,   542,   311,   231, 12350,    35,   461,   231,  5273,\n",
       "          4044,    14,   312,  1652,  2228,   646,  2478,   311,   256,   581,\n",
       "         24607, 43513,   298, 23574,   233,   438, 17443, 16983,     8,    78,\n",
       "            63,   988,    29,  3087,    12,  3122,    29,    16,    14,  1057,\n",
       "             9,   173,   173,   280,   173,  1647,  4508,  1276,  3485,   173,\n",
       "           173,  2745,  1601,   442,   635,   173,  2745,  4855,    14, 11032,\n",
       "           442,  2564,   173,   973, 15673,   978, 36697,   173,     3,   973,\n",
       "         15673,    14,  5349,    63,   961,   978,   438, 17443, 16983,   173,\n",
       "           173,     3,   649,   969,  8102,  2756,   463,  2157,   173, 13404,\n",
       "           233,   635,    14,  2437,    14, 16864,     8,    16,     9,   173,\n",
       "            78,    63,  2461,    63,    17,   233,  4764,   173,    78,    63,\n",
       "          2461,    63,    18,   233,  3038,   173,    56,   233],\n",
       "        [  635,    14,    82,  7285,    17,    14,    21,   408,  9927,    14,\n",
       "         16670,     8,    78,    63,  2461,    63,    17,    12,   554,   491,\n",
       "           880,   443,    14,    21,   408,  9927,    14, 16670,     8,    78,\n",
       "            63,  2461,    63,    18,    12,   554,     9,   382,   404,    18,\n",
       "            12,   554,  1730,   173,    89,   233,   404,    16,    61,   408,\n",
       "           308,    78,    63,  2461,    63,    17,     9,   382,   404,    17,\n",
       "            61,   408,   308,    78,    63,  2461,    63,    18,     9,   173,\n",
       "           173,     3,  2872,   256,   937,   350,   549,   256, 27225, 41476,\n",
       "           173, 16158,   233, 36697,    14, 24607,     8,  5087,   567,  5349,\n",
       "           340,   447,    29,    17,    14,    16,     9,   173, 16158,    14,\n",
       "          2770,     8,    56,    12,   522,     9,   173,   173,    87,   233,\n",
       "         23574,    14,  7602,  7285,    16,    61,   173,    65,   233,   415,\n",
       "            87,    59,    16,    61,   823,   283,    59,    17],\n",
       "        [   61,   173,  3483,   233,   635,    14,  8042,  3226,    21,    12,\n",
       "          1462,     9,   173,  6997,   233,   231,   408, 12017,   415, 23574,\n",
       "            14, 13267,  7285,    16,    61,   823,   283,    59,    17,    61,\n",
       "          4391,   173,     3,   549,   256, 27225, 41476,  1312,  7932,  4025,\n",
       "           173, 15515,   247,   233, 36697,    14, 24607,     8,  5087,   567,\n",
       "          5349,   340,  1149,    63,  2077,  1970,    17,    26,  2009,  1583,\n",
       "           173, 15515,   247,    14,  2770,     8,    56,    12,   522,     9,\n",
       "           173,   173,  2360,   233,   283, 16158,    14,  7602,  7285,    16,\n",
       "            61,   173,   910,   233,   415,  2360,    59,    16,    61,   823,\n",
       "         31005,    59,    17,    61,   173,    87,  6997,   233,  5881,   408,\n",
       "         12017,   415,   283, 16158,    14, 13267,  7285,    16,    61,   823,\n",
       "         31005,    59,    17,    61,   173,   173,     3,  1676, 27225,  7103,\n",
       "         19754,   350,  2751,   173,    72,    16,   233,  2564],\n",
       "        [ 2421,    41,    13,  4609,  1799,  8043,  1942,   480,   461,  2405,\n",
       "          2164,   296,   256,   173,    37,    13,  4609,  1799,  1942,  1314,\n",
       "           461, 22055,  5491,    15,  7098, 11835,   292,   525,  5625,  5921,\n",
       "           173,   973,  1289,  5964,   612,   978,  4381,    63,  2745,    12,\n",
       "           781,    63,  1619,   173,   173,  2745,   756,    14,   419,   173,\n",
       "           173,   973, 36312,   978, 31780,   173,  2745,  4855,    14, 16936,\n",
       "           442, 13444,   173,   173,   973,  5599,  3468,    14, 33978,    14,\n",
       "           838,   978, 27484,  4859, 36664,  4391,   173,  2696,    63,  2841,\n",
       "           233,   396,    14,   173,   173,  3247,    63,  1547,   233,   396,\n",
       "            14,   408,  2814,    63,  2841,   173,  3247,    63,   563,   214,\n",
       "           233,  1911,    14,   408,  2814,    63,  2841,   173,   173,  8537,\n",
       "            63,  5202,   233,  1383, 14714,    63,   441,   340,   269,    73,\n",
       "            63,  4609,  1799,   340,   269,    69,    63,  4593]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out\n",
    "# Shifting the inputs and labels to align them happens inside the model, \n",
    "# so the data collator just copies the inputs to create the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d92c8b70065430ebe21989588ca3c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™ll use a cosine learning rate schedule with some warmup and an effective batch size of 256 (per_device_train_batch_size * gradient_accumulation_steps). Gradient accumulation is used when a single batch does not fit into memory, and incrementally builds up the gradient through several forward/backward passes. Weâ€™ll see this in action when we create the training loop with ðŸ¤— Accelerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/tmp/derek/codeparrot-ds\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    # fp16=True,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hujunhao/.conda/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1381061\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 5394\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='5394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/5394 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hujunhao/CheatSheets/HuggingFace/specific_tasks/code_generation_model.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhujunhao/home/hujunhao/CheatSheets/HuggingFace/specific_tasks/code_generation_model.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py:1521\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1518\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1519\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1520\u001b[0m )\n\u001b[0;32m-> 1521\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1522\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1523\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1524\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1525\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1526\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1761\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1762\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1763\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1765\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1766\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1767\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1768\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1769\u001b[0m ):\n\u001b[1;32m   1770\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py:2517\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2515\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2516\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2517\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2519\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hujunhao/.conda/envs/pytorch/lib/python3.9/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/hujunhao/.conda/envs/pytorch/lib/python3.9/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# create some data\n",
      "x = np.random.randn(100)\n",
      "y = np.random.randn(100)\n",
      "\n",
      "# create scatter plot with x, y\n",
      "plt.scatter(x, y, c='r', marker\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# create some data\n",
      "x = np.random.randn(100)\n",
      "y = np.random.randn(100)\n",
      "\n",
      "# create dataframe from x and y\n",
      "df = pd.DataFrame(x, columns=['x', 'y\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create dataframe from x and y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# dataframe with profession, income and name\n",
      "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
      "\n",
      "# calculate the mean income per profession\n",
      "mean_income = df\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# dataframe with profession, income and name\n",
    "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
    "\n",
    "# calculate the mean income per profession\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# import random forest regressor from scikit-learn\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "# fit random forest model with 300 estimators on X, y:\n",
      "X = np.array([[-7, -6, -0], [\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "# import random forest regressor from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# fit random forest model with 300 estimators on X, y:\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized training loop\n",
    "\n",
    "Sometimes it requires more customization of the model training to achieve the necessary performance for a given use case, however. For example, what if we would like to dynamically update the batch size or have a conditional training loop that skips bad examples on the fly? One option would be to subclass the Trainer and add the necessary changes, but sometimes itâ€™s simpler to write the training loop from scratch. Thatâ€™s where ðŸ¤— Accelerate comes in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0b2c5b007173d0e724d60d424b74ca9c94f227d3b6077b5b6ce4e6aabbbf8fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
