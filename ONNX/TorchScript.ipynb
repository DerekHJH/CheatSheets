{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRODUCTION TO TORCHSCRIPT\n",
    "---\n",
    "\n",
    "This tutorial is an introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu116\n"
     ]
    }
   ],
   "source": [
    "import torch  # This is all you need to use both PyTorch and TorchScript!\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of PyTorch Model Authoring\n",
    "\n",
    "A Module is the basic unit of composition in PyTorch. It contains:\n",
    "\n",
    "1. A constructor, which prepares the module for invocation\n",
    "2. A set of Parameters and sub-Modules. These are initialized by the constructor and can be used by the module during invocation.\n",
    "3. A forward function. This is the code that is run when the module is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCell(\n",
      "  (dg): MyDecisionGate()\n",
      "  (linear): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "(tensor([[ 0.8623,  0.7705,  0.4927,  0.0870],\n",
      "        [ 0.8000,  0.7732, -0.1148,  0.5965],\n",
      "        [ 0.6882,  0.5490, -0.0672,  0.6857]], grad_fn=<TanhBackward0>), tensor([[ 0.8623,  0.7705,  0.4927,  0.0870],\n",
      "        [ 0.8000,  0.7732, -0.1148,  0.5965],\n",
      "        [ 0.6882,  0.5490, -0.0672,  0.6857]], grad_fn=<TanhBackward0>))\n"
     ]
    }
   ],
   "source": [
    "class MyDecisionGate(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return -x\n",
    "\n",
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.dg = MyDecisionGate()\n",
    "        # torch.nn.Linear is a Module from the PyTorch standard library. \n",
    "        # Just like MyCell, it can be invoked using the call syntax. \n",
    "        # We are building a hierarchy of Modules.\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell()\n",
    "print(my_cell)\n",
    "x = torch.rand(3, 4)\n",
    "h = torch.rand(3, 4)\n",
    "print(my_cell(x, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of TorchScript\n",
    "\n",
    "In short, TorchScript provides tools to capture the definition of your model, even in light of the flexible and dynamic nature of PyTorch.\n",
    "\n",
    "## Tracing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCell(\n",
      "  original_name=MyCell\n",
      "  (linear): Linear(original_name=Linear)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1919, -0.3593, -0.0930,  0.6423],\n",
       "         [ 0.2444, -0.4079,  0.4662,  0.2460],\n",
       "         [ 0.0764, -0.6685,  0.1550,  0.6251]], grad_fn=<TanhBackward0>),\n",
       " tensor([[-0.1919, -0.3593, -0.0930,  0.6423],\n",
       "         [ 0.2444, -0.4079,  0.4662,  0.2460],\n",
       "         [ 0.0764, -0.6685,  0.1550,  0.6251]], grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.linear(x) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell()\n",
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "# It has invoked the Module, recorded the operations that occured when the Module was run, \n",
    "# and created an instance of torch.jit.ScriptModule (of which TracedModule is an instance)\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "print(traced_cell)\n",
    "traced_cell(x, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchScript records its definitions in an Intermediate Representation (or IR), commonly referred to in Deep learning as a graph. We can examine the graph with the .graph property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.MyCell,\n",
      "      %x : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu),\n",
      "      %h : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu)):\n",
      "  %linear : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear\"](%self.1)\n",
      "  %20 : Tensor = prim::CallMethod[name=\"forward\"](%linear, %x)\n",
      "  %11 : int = prim::Constant[value=1]() # /tmp/ipykernel_2607982/260609686.py:7:0\n",
      "  %12 : Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu) = aten::add(%20, %h, %11) # /tmp/ipykernel_2607982/260609686.py:7:0\n",
      "  %13 : Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu) = aten::tanh(%12) # /tmp/ipykernel_2607982/260609686.py:7:0\n",
      "  %14 : (Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu), Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu)) = prim::TupleConstruct(%13, %13)\n",
      "  return (%14)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_cell.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is a very low-level representation and most of the information contained in the graph is not useful for end users. Instead, we can use the .code property to give a Python-syntax interpretation of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  linear = self.linear\n",
      "  _0 = torch.tanh(torch.add((linear).forward(x, ), h))\n",
      "  return (_0, _0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_cell.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that invoking traced_cell produces the same results as the Python module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.1919, -0.3593, -0.0930,  0.6423],\n",
      "        [ 0.2444, -0.4079,  0.4662,  0.2460],\n",
      "        [ 0.0764, -0.6685,  0.1550,  0.6251]], grad_fn=<TanhBackward0>), tensor([[-0.1919, -0.3593, -0.0930,  0.6423],\n",
      "        [ 0.2444, -0.4079,  0.4662,  0.2460],\n",
      "        [ 0.0764, -0.6685,  0.1550,  0.6251]], grad_fn=<TanhBackward0>))\n",
      "(tensor([[-0.1919, -0.3593, -0.0930,  0.6423],\n",
      "        [ 0.2444, -0.4079,  0.4662,  0.2460],\n",
      "        [ 0.0764, -0.6685,  0.1550,  0.6251]],\n",
      "       grad_fn=<DifferentiableGraphBackward>), tensor([[-0.1919, -0.3593, -0.0930,  0.6423],\n",
      "        [ 0.2444, -0.4079,  0.4662,  0.2460],\n",
      "        [ 0.0764, -0.6685,  0.1550,  0.6251]],\n",
      "       grad_fn=<DifferentiableGraphBackward>))\n"
     ]
    }
   ],
   "source": [
    "print(my_cell(x, h))\n",
    "print(traced_cell(x, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scripting to Convert Modules\n",
    "\n",
    " Problem with the one with the control-flow-laden submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    argument_1: Tensor) -> Tensor:\n",
      "  return torch.neg(argument_1)\n",
      "\n",
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  dg = self.dg\n",
      "  linear = self.linear\n",
      "  _0 = torch.add((dg).forward((linear).forward(x, ), ), h)\n",
      "  _1 = torch.tanh(_0)\n",
      "  return (_1, _1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/ipykernel_launcher.py:3: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "class MyDecisionGate(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return -x\n",
    "\n",
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self, dg):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.dg = dg\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell(MyDecisionGate())\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "\n",
    "print(traced_cell.dg.code)\n",
    "print(traced_cell.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the .code output, we can see that the if-else branch is nowhere to be found! Why? Tracing does exactly what we said it would: run the code, record the operations that `happen` and construct a ScriptModule that does exactly that. Unfortunately, things like control flow are erased.\n",
    "\n",
    "How can we faithfully represent this module in TorchScript? We provide a script compiler, which does direct analysis of your Python source code to transform it into TorchScript by writing your model in TorchScript directly and annotate your model accordingly. Let’s convert MyDecisionGate using the script compiler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  if bool(torch.gt(torch.sum(x), 0)):\n",
      "    _0 = x\n",
      "  else:\n",
      "    _0 = torch.neg(x)\n",
      "  return _0\n",
      "\n",
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  dg = self.dg\n",
      "  linear = self.linear\n",
      "  _0 = torch.add((dg).forward((linear).forward(x, ), ), h)\n",
      "  new_h = torch.tanh(_0)\n",
      "  return (new_h, new_h)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scripted_gate = torch.jit.script(MyDecisionGate())\n",
    "\n",
    "my_cell = MyCell(scripted_gate)\n",
    "scripted_cell = torch.jit.script(my_cell)\n",
    "\n",
    "print(scripted_gate.code)\n",
    "print(scripted_cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7392, 0.2868, 0.7941, 0.5196],\n",
       "         [0.5743, 0.4229, 0.8638, 0.8357],\n",
       "         [0.5842, 0.4139, 0.8051, 0.5587]],\n",
       "        grad_fn=<DifferentiableGraphBackward>),\n",
       " tensor([[0.7392, 0.2868, 0.7941, 0.5196],\n",
       "         [0.5743, 0.4229, 0.8638, 0.8357],\n",
       "         [0.5842, 0.4139, 0.8051, 0.5587]],\n",
       "        grad_fn=<DifferentiableGraphBackward>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New inputs\n",
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "scripted_cell(x, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixing Scripting and Tracing\n",
    "\n",
    "Some situations call for using tracing rather than scripting (e.g. a module has many architectural decisions that are made based on constant Python values that we would like to not appear in TorchScript). In this case, scripting can be composed with tracing: torch.jit.script will inline the code for a traced module, and tracing will inline the code for a scripted module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    xs: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  h = torch.zeros([3, 4])\n",
      "  y = torch.zeros([3, 4])\n",
      "  y0 = y\n",
      "  h0 = h\n",
      "  for i in range(torch.size(xs, 0)):\n",
      "    cell = self.cell\n",
      "    _0 = (cell).forward(torch.select(xs, 0, i), h0, )\n",
      "    y1, h1, = _0\n",
      "    y0, h0 = y1, h1\n",
      "  return (y0, h0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MyRNNLoop(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRNNLoop, self).__init__()\n",
    "        self.cell = torch.jit.trace(MyCell(scripted_gate), (x, h))\n",
    "\n",
    "    def forward(self, xs):\n",
    "        h, y = torch.zeros(3, 4), torch.zeros(3, 4)\n",
    "        for i in range(xs.size(0)):\n",
    "            y, h = self.cell(xs[i], h)\n",
    "        return y, h\n",
    "\n",
    "rnn_loop = torch.jit.script(MyRNNLoop())\n",
    "print(rnn_loop.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    xs: Tensor) -> Tensor:\n",
      "  loop = self.loop\n",
      "  _0, y, = (loop).forward(xs, )\n",
      "  return torch.relu(y)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class WrapRNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WrapRNN, self).__init__()\n",
    "        self.loop = torch.jit.script(MyRNNLoop())\n",
    "\n",
    "    def forward(self, xs):\n",
    "        y, h = self.loop(xs)\n",
    "        return torch.relu(y)\n",
    "\n",
    "traced = torch.jit.trace(WrapRNN(), (torch.rand(10, 3, 4)))\n",
    "print(traced.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading models\n",
    "\n",
    "We provide APIs to save and load TorchScript modules to/from disk in an archive format. This format includes code, parameters, attributes, and debug information, meaning that the archive is a freestanding representation of the model that can be loaded in an entirely separate process. Let’s save and load our wrapped RNN module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=WrapRNN\n",
      "  (loop): RecursiveScriptModule(\n",
      "    original_name=MyRNNLoop\n",
      "    (cell): RecursiveScriptModule(\n",
      "      original_name=MyCell\n",
      "      (dg): RecursiveScriptModule(original_name=MyDecisionGate)\n",
      "      (linear): RecursiveScriptModule(original_name=Linear)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "def forward(self,\n",
      "    xs: Tensor) -> Tensor:\n",
      "  loop = self.loop\n",
      "  _0, y, = (loop).forward(xs, )\n",
      "  return torch.relu(y)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "traced.save('wrapped_rnn.pt')\n",
    "\n",
    "loaded = torch.jit.load('wrapped_rnn.pt')\n",
    "\n",
    "print(loaded)\n",
    "print(loaded.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, serialization preserves the module hierarchy and the code we’ve been examining throughout. The model can also be loaded, for example, into C++ for python-free execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Your Script Module in C++\n",
    "\n",
    "To load your serialized PyTorch model in C++, your application must depend on the PyTorch C++ API – also known as `LibTorch`. The LibTorch distribution encompasses a collection of shared libraries, header files and CMake build configuration files. While CMake is not a requirement for depending on LibTorch, it is the recommended approach and will be well supported into the future. For this tutorial, we will be building a minimal C++ application using CMake and LibTorch that simply loads and executes a serialized PyTorch model.\n",
    "\n",
    "```C++\n",
    "#include <torch/script.h> // One-stop header.\n",
    "\n",
    "#include <iostream>\n",
    "#include <memory>\n",
    "\n",
    "int main(int argc, const char* argv[]) {\n",
    "  if (argc != 2) {\n",
    "    std::cerr << \"usage: example-app <path-to-exported-script-module>\\n\";\n",
    "    return -1;\n",
    "  }\n",
    "\n",
    "\n",
    "  torch::jit::script::Module module;\n",
    "  try {\n",
    "    // Deserialize the ScriptModule from a file using torch::jit::load().\n",
    "    module = torch::jit::load(argv[1]);\n",
    "  }\n",
    "  catch (const c10::Error& e) {\n",
    "    std::cerr << \"error loading the model\\n\";\n",
    "    return -1;\n",
    "  }\n",
    "\n",
    "  std::cout << \"ok\\n\";\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depending on LibTorch and Building the Application\n",
    "\n",
    "Assume we stored the above code into a file called example-app.cpp. A minimal CMakeLists.txt to build it could look as simple as:\n",
    "\n",
    "```cmake\n",
    "cmake_minimum_required(VERSION 3.0 FATAL_ERROR)\n",
    "project(custom_ops)\n",
    "\n",
    "find_package(Torch REQUIRED)\n",
    "\n",
    "add_executable(example-app example-app.cpp)\n",
    "target_link_libraries(example-app \"${TORCH_LIBRARIES}\")\n",
    "set_property(TARGET example-app PROPERTY CXX_STANDARD 14)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to build the example application is the LibTorch distribution. You can always grab the latest stable release from the download page on the PyTorch website. If you download and unzip the latest archive, you should receive a folder with the following directory structure:\n",
    "\n",
    "```bash\n",
    "libtorch/\n",
    "  bin/\n",
    "  include/\n",
    "  lib/\n",
    "  share/\n",
    "```\n",
    "\n",
    "- The lib/ folder contains the shared libraries you must link against,\n",
    "- The include/ folder contains header files your program will need to include,\n",
    "- The share/ folder contains the necessary CMake configuration to enable the simple find_package(Torch) command above.\n",
    "\n",
    "`On Windows, debug and release builds are not ABI-compatible. If you plan to build your project in debug mode, please try the debug version of LibTorch. Also, make sure you specify the correct configuration in the cmake --build . line below.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is building the application. For this, assume our example directory is laid out like this:\n",
    "```bash\n",
    "example-app/\n",
    "  CMakeLists.txt\n",
    "  example-app.cpp\n",
    "```\n",
    "\n",
    "We can now run the following commands to build the application from within the example-app/ folder:\n",
    "\n",
    "```bash\n",
    "mkdir build\n",
    "cd build\n",
    "cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n",
    "cmake --build . --config Release\n",
    "```\n",
    "\n",
    "where /path/to/libtorch should be the full path to the unzipped LibTorch distribution. If all goes well, it will look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "root@4b5a67132e81:/example-app# mkdir build\n",
    "root@4b5a67132e81:/example-app# cd build\n",
    "root@4b5a67132e81:/example-app/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n",
    "-- The C compiler identification is GNU 5.4.0\n",
    "-- The CXX compiler identification is GNU 5.4.0\n",
    "-- Check for working C compiler: /usr/bin/cc\n",
    "-- Check for working C compiler: /usr/bin/cc -- works\n",
    "-- Detecting C compiler ABI info\n",
    "-- Detecting C compiler ABI info - done\n",
    "-- Detecting C compile features\n",
    "-- Detecting C compile features - done\n",
    "-- Check for working CXX compiler: /usr/bin/c++\n",
    "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
    "-- Detecting CXX compiler ABI info\n",
    "-- Detecting CXX compiler ABI info - done\n",
    "-- Detecting CXX compile features\n",
    "-- Detecting CXX compile features - done\n",
    "-- Looking for pthread.h\n",
    "-- Looking for pthread.h - found\n",
    "-- Looking for pthread_create\n",
    "-- Looking for pthread_create - not found\n",
    "-- Looking for pthread_create in pthreads\n",
    "-- Looking for pthread_create in pthreads - not found\n",
    "-- Looking for pthread_create in pthread\n",
    "-- Looking for pthread_create in pthread - found\n",
    "-- Found Threads: TRUE\n",
    "-- Configuring done\n",
    "-- Generating done\n",
    "-- Build files have been written to: /example-app/build\n",
    "root@4b5a67132e81:/example-app/build# make\n",
    "Scanning dependencies of target example-app\n",
    "[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o\n",
    "[100%] Linking CXX executable example-app\n",
    "[100%] Built target example-app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "root@4b5a67132e81:/example-app/build# ./example-app <path_to_model>/traced_resnet_model.pt\n",
    "ok\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Script Module in C++\n",
    "\n",
    "Having successfully loaded our serialized `ResNet18` in C++, we are now just a couple lines of code away from executing it! Let’s add those lines to our C++ application’s `main()` function:\n",
    "\n",
    "```C++\n",
    "// Create a vector of inputs.\n",
    "std::vector<torch::jit::IValue> inputs;\n",
    "// torch::jit::IValue is a type-erased value type script::Module methods accept and return\n",
    "inputs.push_back(torch::ones({1, 3, 224, 224}));\n",
    "\n",
    "// Execute the model and turn its output into a tensor.\n",
    "at::Tensor output = module.forward(inputs).toTensor();\n",
    "// module.forward(inputs) is a torch::jit::IValue\n",
    "std::cout << output.slice(/*dim=*/1, /*start=*/0, /*end=*/5) << '\\n';\n",
    "// print the first five entries of the output.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-compile our application and run it with the same serialized model.\n",
    "```bash\n",
    "root@4b5a67132e81:/example-app/build# make\n",
    "Scanning dependencies of target example-app\n",
    "[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o\n",
    "[100%] Linking CXX executable example-app\n",
    "[100%] Built target example-app\n",
    "root@4b5a67132e81:/example-app/build# ./example-app traced_resnet_model.pt\n",
    "-0.2698 -0.0381  0.4023 -0.3010 -0.0448\n",
    "[ Variable[CPUFloatType]{1,5} ]\n",
    "```\n",
    "\n",
    "Tips: `To move your model to GPU memory, you can write model.to(at::kCUDA);. Make sure the inputs to a model are also living in CUDA memory by calling tensor.to(at::kCUDA), which will return a new tensor in CUDA memory.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclsion\n",
    "\n",
    "With the concepts described in this tutorial, you should be able to go from a vanilla, “eager” PyTorch model, to a compiled ScriptModule in Python, to a serialized file on disk and – to close the loop – to an executable script::Module in C++.\n",
    "\n",
    "Of course, there are many concepts we did not cover. For example, you may find yourself wanting to extend your ScriptModule with a custom operator implemented in C++ or CUDA, and executing this custom operator inside your ScriptModule loaded in your pure C++ production environment. The good news is: this is possible, and well supported! For now, you can explore [this](https://github.com/pytorch/pytorch/tree/master/test/custom_operator) folder for examples, and we will follow up with a tutorial shortly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('onnx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6cd07bb6952f6cc6d5196ff2c866fd68acf6adb1fa9074d49a10859dae1b8c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
