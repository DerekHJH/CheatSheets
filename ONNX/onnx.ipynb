{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch.ONNX\n",
    "---\n",
    "\n",
    "Open Neural Network eXchange (ONNX) is an open standard format for representing machine learning models. The torch.onnx module can export PyTorch models to ONNX. The model can then be consumed by any of the many [runtimes that support ONNX](https://onnx.ai/supported-tools.html#deployModel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: AlexNet from PyTorch to ONNX\n",
    "\n",
    "Here is a simple script which exports a pretrained AlexNet to an ONNX file named alexnet.onnx. The call to torch.onnx.export runs the model once to trace its execution and then exports the traced model to the specified file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /home/hujunhao/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%actual_input_1 : Float(10, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0),\n",
      "      %learned_0 : Float(64, 3, 11, 11, strides=[363, 121, 11, 1], requires_grad=1, device=cuda:0),\n",
      "      %learned_1 : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %learned_2 : Float(192, 64, 5, 5, strides=[1600, 25, 5, 1], requires_grad=1, device=cuda:0),\n",
      "      %learned_3 : Float(192, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %learned_4 : Float(384, 192, 3, 3, strides=[1728, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %learned_5 : Float(384, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %learned_6 : Float(256, 384, 3, 3, strides=[3456, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %learned_7 : Float(256, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %learned_8 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %learned_9 : Float(256, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %learned_10 : Float(4096, 9216, strides=[9216, 1], requires_grad=1, device=cuda:0),\n",
      "      %learned_11 : Float(4096, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %learned_12 : Float(4096, 4096, strides=[4096, 1], requires_grad=1, device=cuda:0),\n",
      "      %learned_13 : Float(4096, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %learned_14 : Float(1000, 4096, strides=[4096, 1], requires_grad=1, device=cuda:0),\n",
      "      %learned_15 : Float(1000, strides=[1], requires_grad=1, device=cuda:0)):\n",
      "  %/features/features.0/Conv_output_0 : Float(10, 64, 55, 55, strides=[193600, 3025, 55, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4], onnx_name=\"/features/features.0/Conv\"](%actual_input_1, %learned_0, %learned_1), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.0 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/modules/conv.py:460:0\n",
      "  %/features/features.1/Relu_output_0 : Float(10, 64, 55, 55, strides=[193600, 3025, 55, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/features/features.1/Relu\"](%/features/features.0/Conv_output_0), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.1 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/features/features.2/MaxPool_output_0 : Float(10, 64, 27, 27, strides=[46656, 729, 27, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[ceil_mode=0, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/features/features.2/MaxPool\"](%/features/features.1/Relu_output_0), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.MaxPool2d::features.2 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/functional.py:782:0\n",
      "  %/features/features.3/Conv_output_0 : Float(10, 192, 27, 27, strides=[139968, 729, 27, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1], onnx_name=\"/features/features.3/Conv\"](%/features/features.2/MaxPool_output_0, %learned_2, %learned_3), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.3 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/modules/conv.py:460:0\n",
      "  %/features/features.4/Relu_output_0 : Float(10, 192, 27, 27, strides=[139968, 729, 27, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/features/features.4/Relu\"](%/features/features.3/Conv_output_0), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.4 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/features/features.5/MaxPool_output_0 : Float(10, 192, 13, 13, strides=[32448, 169, 13, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[ceil_mode=0, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/features/features.5/MaxPool\"](%/features/features.4/Relu_output_0), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.MaxPool2d::features.5 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/functional.py:782:0\n",
      "  %/features/features.6/Conv_output_0 : Float(10, 384, 13, 13, strides=[64896, 169, 13, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/features/features.6/Conv\"](%/features/features.5/MaxPool_output_0, %learned_4, %learned_5), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.6 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/modules/conv.py:460:0\n",
      "  %/features/features.7/Relu_output_0 : Float(10, 384, 13, 13, strides=[64896, 169, 13, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/features/features.7/Relu\"](%/features/features.6/Conv_output_0), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.7 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/features/features.8/Conv_output_0 : Float(10, 256, 13, 13, strides=[43264, 169, 13, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/features/features.8/Conv\"](%/features/features.7/Relu_output_0, %learned_6, %learned_7), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.8 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/modules/conv.py:460:0\n",
      "  %/features/features.9/Relu_output_0 : Float(10, 256, 13, 13, strides=[43264, 169, 13, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/features/features.9/Relu\"](%/features/features.8/Conv_output_0), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.9 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/features/features.10/Conv_output_0 : Float(10, 256, 13, 13, strides=[43264, 169, 13, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/features/features.10/Conv\"](%/features/features.9/Relu_output_0, %learned_8, %learned_9), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.10 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/modules/conv.py:460:0\n",
      "  %/features/features.11/Relu_output_0 : Float(10, 256, 13, 13, strides=[43264, 169, 13, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/features/features.11/Relu\"](%/features/features.10/Conv_output_0), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.11 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/features/features.12/MaxPool_output_0 : Float(10, 256, 6, 6, strides=[9216, 36, 6, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[ceil_mode=0, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/features/features.12/MaxPool\"](%/features/features.11/Relu_output_0), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.MaxPool2d::features.12 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/functional.py:782:0\n",
      "  %/avgpool/AveragePool_output_0 : Float(10, 256, 6, 6, strides=[9216, 36, 6, 1], requires_grad=1, device=cuda:0) = onnx::AveragePool[kernel_shape=[1, 1], strides=[1, 1], onnx_name=\"/avgpool/AveragePool\"](%/features/features.12/MaxPool_output_0), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.pooling.AdaptiveAvgPool2d::avgpool # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/functional.py:1214:0\n",
      "  %/Flatten_output_0 : Float(10, 9216, strides=[9216, 1], requires_grad=1, device=cuda:0) = onnx::Flatten[axis=1, onnx_name=\"/Flatten\"](%/avgpool/AveragePool_output_0), scope: torchvision.models.alexnet.AlexNet:: # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torchvision/models/alexnet.py:50:0\n",
      "  %/classifier/classifier.1/Gemm_output_0 : Float(10, 4096, strides=[4096, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/classifier/classifier.1/Gemm\"](%/Flatten_output_0, %learned_10, %learned_11), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::classifier/torch.nn.modules.linear.Linear::classifier.1 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/classifier/classifier.2/Relu_output_0 : Float(10, 4096, strides=[4096, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/classifier/classifier.2/Relu\"](%/classifier/classifier.1/Gemm_output_0), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::classifier/torch.nn.modules.activation.ReLU::classifier.2 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/classifier/classifier.4/Gemm_output_0 : Float(10, 4096, strides=[4096, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/classifier/classifier.4/Gemm\"](%/classifier/classifier.2/Relu_output_0, %learned_12, %learned_13), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::classifier/torch.nn.modules.linear.Linear::classifier.4 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/classifier/classifier.5/Relu_output_0 : Float(10, 4096, strides=[4096, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/classifier/classifier.5/Relu\"](%/classifier/classifier.4/Gemm_output_0), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::classifier/torch.nn.modules.activation.ReLU::classifier.5 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %output1 : Float(10, 1000, strides=[1000, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/classifier/classifier.6/Gemm\"](%/classifier/classifier.5/Relu_output_0, %learned_14, %learned_15), scope: torchvision.models.alexnet.AlexNet::/torch.nn.modules.container.Sequential::classifier/torch.nn.modules.linear.Linear::classifier.6 # /home/hujunhao/.conda/envs/onnx/lib/python3.7/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  return (%output1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(10, 3, 224, 224, device=\"cuda\")\n",
    "model = torchvision.models.alexnet(pretrained=True).cuda()\n",
    "\n",
    "# Providing input and output names sets the display names for values\n",
    "# within the model's graph. \n",
    "#\n",
    "# The inputs to the network consist of the flat list of inputs followed by the\n",
    "# flat list of parameters. You can partially specify names, i.e. provide\n",
    "# a list here shorter than the number of inputs to the model, and we will\n",
    "# only set that subset of names, starting from the beginning.\n",
    "input_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\n",
    "output_names = [ \"output1\" ]\n",
    "\n",
    "torch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, \\\n",
    "    input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting alexnet.onnx file contains a binary protocol buffer which contains both the network structure and parameters of the model you exported (in this case, AlexNet). The argument verbose=True causes the exporter to print out a human-readable representation of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# These are the inputs and parameters to the network, which have taken on\n",
    "# the names we specified earlier.\n",
    "graph(%actual_input_1 : Float(10, 3, 224, 224)\n",
    "      %learned_0 : Float(64, 3, 11, 11)\n",
    "      %learned_1 : Float(64)\n",
    "      %learned_2 : Float(192, 64, 5, 5)\n",
    "      %learned_3 : Float(192)\n",
    "      # ---- omitted for brevity ----\n",
    "      %learned_14 : Float(1000, 4096)\n",
    "      %learned_15 : Float(1000)) {\n",
    "  # Every statement consists of some output tensors (and their types),\n",
    "  # the operator to be run (with its attributes, e.g., kernels, strides,\n",
    "  # etc.), its input tensors (%actual_input_1, %learned_0, %learned_1)\n",
    "  %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0]\n",
    "  %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n",
    "  %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n",
    "  # ---- omitted for brevity ----\n",
    "  %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n",
    "  # Dynamic means that the shape is not known. This may be because of a\n",
    "  # limitation of our implementation (which we would like to fix in a\n",
    "  # future release) or shapes which are truly dynamic.\n",
    "  %30 : Dynamic = onnx::Shape(%29), scope: AlexNet\n",
    "  %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet\n",
    "  %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet\n",
    "  %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet\n",
    "  # ---- omitted for brevity ----\n",
    "  %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6]\n",
    "  return (%output1);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also verify the output using the ONNX library, which you can install using pip:\n",
    "```bash\n",
    "pip install onnx\n",
    "```\n",
    "\n",
    "Then, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch_jit (\n",
      "  %actual_input_1[FLOAT, 10x3x224x224]\n",
      ") initializers (\n",
      "  %learned_0[FLOAT, 64x3x11x11]\n",
      "  %learned_1[FLOAT, 64]\n",
      "  %learned_2[FLOAT, 192x64x5x5]\n",
      "  %learned_3[FLOAT, 192]\n",
      "  %learned_4[FLOAT, 384x192x3x3]\n",
      "  %learned_5[FLOAT, 384]\n",
      "  %learned_6[FLOAT, 256x384x3x3]\n",
      "  %learned_7[FLOAT, 256]\n",
      "  %learned_8[FLOAT, 256x256x3x3]\n",
      "  %learned_9[FLOAT, 256]\n",
      "  %learned_10[FLOAT, 4096x9216]\n",
      "  %learned_11[FLOAT, 4096]\n",
      "  %learned_12[FLOAT, 4096x4096]\n",
      "  %learned_13[FLOAT, 4096]\n",
      "  %learned_14[FLOAT, 1000x4096]\n",
      "  %learned_15[FLOAT, 1000]\n",
      ") {\n",
      "  %/features/features.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [11, 11], pads = [2, 2, 2, 2], strides = [4, 4]](%actual_input_1, %learned_0, %learned_1)\n",
      "  %/features/features.1/Relu_output_0 = Relu(%/features/features.0/Conv_output_0)\n",
      "  %/features/features.2/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [2, 2]](%/features/features.1/Relu_output_0)\n",
      "  %/features/features.3/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%/features/features.2/MaxPool_output_0, %learned_2, %learned_3)\n",
      "  %/features/features.4/Relu_output_0 = Relu(%/features/features.3/Conv_output_0)\n",
      "  %/features/features.5/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [2, 2]](%/features/features.4/Relu_output_0)\n",
      "  %/features/features.6/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/features/features.5/MaxPool_output_0, %learned_4, %learned_5)\n",
      "  %/features/features.7/Relu_output_0 = Relu(%/features/features.6/Conv_output_0)\n",
      "  %/features/features.8/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/features/features.7/Relu_output_0, %learned_6, %learned_7)\n",
      "  %/features/features.9/Relu_output_0 = Relu(%/features/features.8/Conv_output_0)\n",
      "  %/features/features.10/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/features/features.9/Relu_output_0, %learned_8, %learned_9)\n",
      "  %/features/features.11/Relu_output_0 = Relu(%/features/features.10/Conv_output_0)\n",
      "  %/features/features.12/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [2, 2]](%/features/features.11/Relu_output_0)\n",
      "  %/avgpool/AveragePool_output_0 = AveragePool[kernel_shape = [1, 1], strides = [1, 1]](%/features/features.12/MaxPool_output_0)\n",
      "  %/Flatten_output_0 = Flatten[axis = 1](%/avgpool/AveragePool_output_0)\n",
      "  %/classifier/classifier.1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/Flatten_output_0, %learned_10, %learned_11)\n",
      "  %/classifier/classifier.2/Relu_output_0 = Relu(%/classifier/classifier.1/Gemm_output_0)\n",
      "  %/classifier/classifier.4/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/classifier.2/Relu_output_0, %learned_12, %learned_13)\n",
      "  %/classifier/classifier.5/Relu_output_0 = Relu(%/classifier/classifier.4/Gemm_output_0)\n",
      "  %output1 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/classifier.5/Relu_output_0, %learned_14, %learned_15)\n",
      "  return %output1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load the ONNX model\n",
    "model = onnx.load(\"alexnet.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run the exported model with one of the many runtimes that support ONNX. For example after installing ONNX Runtime, you can load and run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.29637927 -1.608035   -1.4394506  ... -1.1821014  -1.1421627\n",
      "   1.2509004 ]\n",
      " [ 0.23148066 -1.41478    -1.4067262  ... -1.4743594  -1.0560268\n",
      "   0.89103293]\n",
      " [-0.02423095 -1.1773117  -1.5052799  ... -1.2808325  -0.9275118\n",
      "   1.207436  ]\n",
      " ...\n",
      " [-0.33603638 -2.08741    -1.8960619  ... -1.4387039  -1.1724287\n",
      "   1.397223  ]\n",
      " [ 0.27008438 -1.3138564  -1.3698235  ... -1.0562149  -1.1480815\n",
      "   0.9472613 ]\n",
      " [ 0.0924949  -1.3592756  -1.4721488  ... -1.375487   -0.8026843\n",
      "   0.90579134]]\n"
     ]
    }
   ],
   "source": [
    "ort_session = ort.InferenceSession(\"alexnet.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {\"actual_input_1\": np.random.randn(10, 3, 224, 224).astype(np.float32)},\n",
    ")\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing vs Scripting\n",
    "\n",
    "Internally, torch.onnx.export() requires a torch.jit.ScriptModule rather than a torch.nn.Module. If the passed-in model is not already a ScriptModule, export() will use tracing to convert it to one:\n",
    "\n",
    "- Tracing: If torch.onnx.export() is called with a Module that is not already a ScriptModule, it first does the equivalent of torch.jit.trace(), which executes the model once with the given args and records all operations that happen during that execution. This means that if your model is dynamic, e.g., changes behavior depending on input data, the exported model will not capture this dynamic behavior. We recommend examining the exported model and making sure the operators look reasonable. Tracing will unroll loops and if statements, exporting a static graph that is exactly the same as the traced run. If you want to export your model with dynamic control flow, you will need to use scripting.\n",
    "\n",
    "- Scripting: Compiling a model via scripting preserves dynamic control flow and is valid for inputs of different sizes. To use scripting:\n",
    "    * Use torch.jit.script() to produce a ScriptModule.\n",
    "    * Call torch.onnx.export() with the ScriptModule as the model. The args are still required, but they will be used internally only to produce example outputs, so that the types and shapes of the outputs can be captured. No tracing will be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Pitfalls\n",
    "\n",
    "## Avoid NumPy and built-in Python types\n",
    "\n",
    "PyTorch models can be written using NumPy or Python types and `functions`, but during tracing, any variables of NumPy or Python types (rather than torch.Tensor) are converted to constants, which will produce the wrong result if those values should change depending on the inputs.\n",
    "\n",
    "For example, rather than using numpy functions on numpy.ndarrays:\n",
    "```python\n",
    "# Bad! Will be replaced with constants during tracing.\n",
    "x, y = np.random.rand(1, 2), np.random.rand(1, 2)\n",
    "np.concatenate((x, y), axis=1)\n",
    "```\n",
    "\n",
    "Use torch operators on torch.Tensors:\n",
    "```python\n",
    "# Good! Tensor operations will be captured during tracing.\n",
    "x, y = torch.randn(1, 2), torch.randn(1, 2)\n",
    "torch.cat((x, y), dim=1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And rather than use torch.Tensor.item() (which converts a Tensor to a Python built-in number):\n",
    "\n",
    "```python\n",
    "# Bad! y.item() will be replaced with a constant during tracing.\n",
    "def forward(self, x, y):\n",
    "    return x.reshape(y.item(), -1)\n",
    "```\n",
    "\n",
    "Use torchâ€™s support for implicit casting of single-element tensors:\n",
    "\n",
    "```python\n",
    "# Good! y will be preserved as a variable during tracing.\n",
    "def forward(self, x, y):\n",
    "    return x.reshape(y, -1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid Tensor.data\n",
    "\n",
    "Using the Tensor.data field can produce an incorrect trace and therefore an incorrect ONNX graph. Use torch.Tensor.detach() instead. (Work is ongoing to remove Tensor.data entirely)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid in-place operations when using tensor.shape in tracing mode\n",
    "\n",
    "In tracing mode, shapes obtained from tensor.shape are traced as tensors, and share the same memory. This might cause a mismatch the final output values. As a workaround, avoid the use of inplace operations in these scenarios. For example, in the model:\n",
    "\n",
    "```python\n",
    "class Model(torch.nn.Module):\n",
    "  def forward(self, states):\n",
    "      batch_size, seq_length = states.shape[:2]\n",
    "      real_seq_length = seq_length\n",
    "      real_seq_length += 2\n",
    "      return real_seq_length + seq_length\n",
    "```\n",
    "real_seq_length and seq_length share the same memory in tracing mode. This could be avoided by rewriting the inplace operation:\n",
    "\n",
    "```python\n",
    "real_seq_length = real_seq_length + 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "\n",
    "## Type\n",
    "\n",
    "- Only torch.Tensors, numeric types that can be trivially converted to torch.Tensors (e.g. float, int), and tuples and lists of those types are supported as model inputs or outputs. Dict and str inputs and outputs are accepted in tracing mode, but:\n",
    "    - Any computation that depends on the value of a dict or a str input will be replaced with the constant value seen during the one traced execution.\n",
    "    - Any output that is a dict will be silently replaced with a flattened sequence of its values (keys will be removed). E.g. {\"foo\": 1, \"bar\": 2} becomes (1, 2).\n",
    "    - Any output that is a str will be silently removed.\n",
    "- Certain operations involving tuples and lists are not supported in scripting mode due to limited support in ONNX for nested sequences. In particular appending a tuple to a list is not supported. In tracing mode, the nested sequences will be flattened automatically during the tracing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences in Operator Implementations\n",
    "\n",
    "Due to differences in implementations of operators, running the exported model on different runtimes may produce different results from each other or from PyTorch. Normally these differences are numerically small, so this should only be a concern if your application is sensitive to these small differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupported Tensor Indexing Patterns\n",
    "\n",
    "Tensor indexing patterns that cannot be exported are listed below. If you are experiencing issues exporting a model that does not include any of the unsupported patterns below, please double check that you are exporting with the latest `opset_version`.\n",
    "\n",
    "### Reads / Gets\n",
    "\n",
    "When indexing into a tensor for reading, the following patterns are not supported:\n",
    "\n",
    "```python\n",
    "# Tensor indices that includes negative values.\n",
    "data[torch.tensor([[1, 2], [2, -3]]), torch.tensor([-2, 3])]\n",
    "# Workarounds: use positive index values.\n",
    "```\n",
    "\n",
    "### Writes / Sets\n",
    "\n",
    "When indexing into a Tensor for writing, the following patterns are not supported:\n",
    "\n",
    "```python\n",
    "# Multiple tensor indices if any has rank >= 2\n",
    "data[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])] = new_data\n",
    "# Workarounds: use single tensor index with rank >= 2,\n",
    "#              or multiple consecutive tensor indices with rank == 1.\n",
    "\n",
    "# Multiple tensor indices that are not consecutive\n",
    "data[torch.tensor([2, 3]), :, torch.tensor([1, 2])] = new_data\n",
    "# Workarounds: transpose `data` such that tensor indices are consecutive.\n",
    "\n",
    "# Tensor indices that includes negative values.\n",
    "data[torch.tensor([1, -2]), torch.tensor([-2, 3])] = new_data\n",
    "# Workarounds: use positive index values.\n",
    "\n",
    "# Implicit broadcasting required for new_data.\n",
    "data[torch.tensor([[0, 2], [1, 1]]), 1:3] = new_data\n",
    "# Workarounds: expand new_data explicitly.\n",
    "# Example:\n",
    "#   data shape: [3, 4, 5]\n",
    "#   new_data shape: [5]\n",
    "#   expected new_data shape after broadcasting: [2, 2, 2, 5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding support for operators\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('onnx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6cd07bb6952f6cc6d5196ff2c866fd68acf6adb1fa9074d49a10859dae1b8c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
